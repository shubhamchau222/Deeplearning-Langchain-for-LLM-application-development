Example generation
Manual evaluation (and debuging)
LLM-assisted evaluation
LangChain evaluation platform


1) Manual evaluation:
	- generate some manual Question answer pair based on Knowledge Source
	eg; 
		examples = [
	    {
	        "query": "Do the Cozy Comfort Pullover Set\
	        have side pockets?",
	        "answer": "Yes"
	    },
	    {
	        "query": "What collection is the Ultra-Lofty \
	        850 Stretch Down Hooded Jacket from?",
	        "answer": "The DownTek collection"
	    }
	]

	## folliwing code will allow to generate automatically QA generation set based on Given document

	from langchain.evaluation.qa import QAGenerateChain
	example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))
	new_examples = example_gen_chain.apply_and_parse(
									    [{"doc": t} for t in data[:5]]
										)

	new_examples[0]
		----> 
			{'query': " According to the document, what is the approximate weight of the Women's Campside Oxfords per pair?\n\n",
		 'answer': " The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz."}



 	## create app & test 
	llm = ChatOpenAI(temperature = 0.0, model=llm_model)
	qa = RetrievalQA.from_chain_type(
									llm=llm, 
									chain_type="stuff", 
									retriever=index.vectorstore.as_retriever(), 
									verbose=True,
									chain_type_kwargs = {
														"document_separator": "<<<<>>>>>"
																}
										)

	## Model Evaluation ***************************************

	import langchain
	langchain.debug = True
	qa.run(examples[0]["query"]) ## will return query, context & ans

	# Turn off the debug mode
	langchain.debug = False

##### ************************************************************************************************************************

2) LLM Assisted Evaluation:


	predictions = qa.apply(examples) ## will return  context, query, op

	from langchain.evaluation.qa import QAEvalChain
	llm = ChatOpenAI(temperature=0, model=llm_model)
	eval_chain = QAEvalChain.from_llm(llm)

	graded_outputs = eval_chain.evaluate(examples, predictions) ## compaires the manual QA sample with model predictions

	for i, eg in enumerate(examples):
	    print(f"Example {i}:")
	    print("Question: " + predictions[i]['query'])
	    print("Real Answer: " + predictions[i]['answer'])
	    print("Predicted Answer: " + predictions[i]['result'])
	    print("Predicted Grade: " + graded_outputs[i]['text'])
	    print()

	## Answers
	Example 0:
	Question: Do the Cozy Comfort Pullover Set        have side pockets?
	Real Answer: Yes
	Predicted Answer: Yes, the Cozy Comfort Pullover Set does have side pockets.
	Predicted Grade: CORRECT

	Example 1:
	Question: What collection is the Ultra-Lofty         850 Stretch Down Hooded Jacket from?
	Real Answer: The DownTek collection
	Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.
	Predicted Grade: CORRECT

	graded_outputs[0]
	## {'text': 'correct'}








	


