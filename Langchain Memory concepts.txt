Memory:

LLM doesnt have the memory state, to store the conversation history:

hence, Langchain provides 4 types of memory;
	1) ConversationBufferMemory
	2) ConversationBufferWindowMemory
	3) ConversationTokenBufferMemory
	4) ConverationSummaryMemory

# other memory types:
	- Vector data memory
	- entity memories
	- you can also use multiple memories at a time

1) ConversationBufferMemory: 
	from langchain.chat_models import ChatOpenAI
	from langchain.chains import ConversationChain
	from langchain.memory import ConversationBufferMemory


	llm = ChatOpenAI(temperature=0.0, model=llm_model)
	memory = ConversationBufferMemory()
	conversation = ConversationChain(
	    llm=llm, 
 	   memory = memory,
	    verbose=True
	)

	
   # conversation.predict(input="Hi, my name is Andrew")
	conversation.predict(input="What is my name?")
	print(memory.buffer)
	memory.load_memory_variables({}) # to check the memory content


	- The chat is contineously getting stored into the conversational buffer memory
	- LLM uses the conversational history to generate the output, inthis way it can remember the previous context
	- As we keep going, memory storage getting huge and huge, and hence model input also increase which causes high charges during prediction
	- As LLM charge based on token counts, so it's very expensive to work with long buffer memory	


- so langchain provide, Conversational Buffer window to tackle this issue;


2) ConversationalBufferWindowMemory:

	from langchain.memory import ConversationBufferWindowMemory
	memory = ConversationBufferWindowMemory(k=1)
	# here k means, window size (how many passed conversation to remember)

	llm = ChatOpenAI(temperature=0.0, model=llm_model)
	memory = ConversationBufferWindowMemory(k=1)
	conversation = ConversationChain(
    llm=llm, 
    memory = memory,
    verbose=False
	)

	# memory, will remember only recent conversation, inthis way we can handle the memory storage & models reference input
	# selection of window size is totally depends on usecase & it's experimental parameter


3) ConversationTokenBufferMemory:

	# In this method: Memory will limit the number of tokens saved, because OpenAI / LLM models charge Based on Tokens

	from langchain.memory import ConversationTokenBufferMemory
	from langchain.llms import OpenAI
	llm = ChatOpenAI(temperature=0.0, model=llm_model)

	memory = ConversationTokenBufferMemory(llm=llm, 
										max_token_limit=50
										)
	##here, Buffer memory only remember/store recent 50 tokens only; 

	memory.save_context({"input": "AI is what?!"},
	            {"output": "Amazing!"})
	memory.save_context({"input": "Backpropagation is what?"},
	            {"output": "Beautiful!"})
	memory.save_context({"input": "Chatbots are what?"}, 
	            {"output": "Charming!"})


4) ConversationSummaryBufferMemory
	## let's use the LLM to make the summary of the conversation & store this in BufferMemory

	from langchain.memory import ConversationSummaryBufferMemory
	memory = ConversationSummaryBufferMemory(llm=llm, 
											max_token_limit=100
											)

	memory.save_context({"input": "Hello"}, {"output": "What's up"})
	memory.save_context({"input": "Not much, just hanging"},
	                    {"output": "Cool"})
	memory.save_context({"input": "What is on the schedule today?"}, 
	                    {"output": f"{schedule}"})


               

	



